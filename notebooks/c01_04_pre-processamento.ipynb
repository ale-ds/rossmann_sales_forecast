{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS E CONFIGURAÇÕES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from boruta import BorutaPy\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (20, 16)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pkl.load( open( \"../data/cleansed/df_cleansed.pkl\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRÉ-PORCESSAMENTO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformação dos Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rescaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **RobustScaler**: Foi escolhido para CompetitionDistance e CompetitionTimeMonth porque esses atributos apresentam outliers. O RobustScaler usa a mediana e os quartis para realizar o escalonamento, tornando-o menos sensível a valores extremos. Isso é útil para preservar a informação presente na maioria dos dados, sem que os outliers distorçam a escala.\n",
    "\n",
    "> **MinMaxScaler**: Foi escolhido para PromoTimeWeek e Year porque o objetivo é trazer os valores para dentro de um intervalo fixo (0 a 1). O MinMaxScaler é sensível a outliers, mas como o objetivo é apenas garantir que os valores estejam em uma determinada escala, e não necessariamente preservar a distribuição original, ele se torna uma boa opção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Rescaling ---\n",
    "path_preprocessing = '../model/pre-processing'\n",
    "# RobustScaler para features com outliers\n",
    "rs_comp_dist = RobustScaler()\n",
    "df1['CompetitionDistance'] = rs_comp_dist.fit_transform(df1[['CompetitionDistance']].values)\n",
    "\n",
    "rs_comp_time_month = RobustScaler()\n",
    "df1['CompetitionTimeMonth'] = rs_comp_time_month.fit_transform(df1[['CompetitionTimeMonth']].values)\n",
    "\n",
    "# MinMaxScaler para as demais features\n",
    "mms_promo_time_week = MinMaxScaler()\n",
    "df1['PromoTimeWeek'] = mms_promo_time_week.fit_transform(df1[['PromoTimeWeek']].values)\n",
    "\n",
    "mms_year = MinMaxScaler()\n",
    "df1['Year'] = mms_year.fit_transform(df1[['Year']].values)\n",
    "\n",
    "mms_promo2_since_year = MinMaxScaler()\n",
    "df1['Promo2SinceYear'] = mms_promo2_since_year.fit_transform(df1[['Promo2SinceYear']].values)\n",
    "\n",
    "mms_comp_open_since_year = MinMaxScaler()\n",
    "df1['CompetitionOpenSinceYear'] = mms_comp_open_since_year.fit_transform(df1[['CompetitionOpenSinceYear']].values)\n",
    "\n",
    "mms_comp_open_since_month = MinMaxScaler()\n",
    "df1['CompetitionOpenSinceMonth'] = mms_comp_open_since_month.fit_transform(df1[['CompetitionOpenSinceMonth']].values)\n",
    "\n",
    "mms_promo2_since_week = MinMaxScaler()\n",
    "df1['Promo2SinceWeek'] = mms_promo2_since_week.fit_transform(df1[['Promo2SinceWeek']].values)\n",
    "\n",
    "df1['Promo'] = df1['Promo'].astype(int)\n",
    "df1['Promo2'] = df1['Promo2'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **StateHoliday e StoreType**, utiliza-se a técnica de One-Hot Encoding devido à inexistência de ordem ou relação hierárquica entre as categorias. Cada categoria (feriado ou tipo de loja) é tratada como uma entidade independente, e o One-Hot Encoding garante que o modelo não interprete erroneamente uma relação ordinal inexistente entre elas, criando colunas binárias distintas para cada categoria.\n",
    "\n",
    "> **Assortment** - Ordinal Encoding: Ordinal encoding é apropriado aqui porque existe uma ordem clara entre os tipos de sortimento: básico < extra < estendido. O mapeamento para 1, 2 e 3 preserva essa ordem, permitindo que o modelo capture a relação entre os diferentes níveis de sortimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Assortment\n",
       "1    444875\n",
       "3    391254\n",
       "2      8209\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# StateHoliday - One Hot Encoding\n",
    "# state_holiday é um estado, ou é feriado ou não é feriado,\n",
    "df1 = pd.get_dummies(df1, prefix=['StateHoliday'], columns=['StateHoliday'] )\n",
    "\n",
    "# StoreType - Label Encoding\n",
    "# Como store_type não possui uma ordem, podemos usar o LabelEncoder\n",
    "le_store_type = LabelEncoder()\n",
    "df1['StoreType'] = le_store_type.fit_transform( df1['StoreType'])\n",
    "pkl.dump(le_store_type, open(f'{path_preprocessing}/label_encoder_store_type.pkl', 'wb'))\n",
    "# Assortment - Ordinal Encoding\n",
    "# Assortment possui ordem, basic < extra < extended\n",
    "assortmentDict = {'basic': 1,  'extra': 2, 'extended': 3}\n",
    "df1['Assortment'] = df1['Assortment'].map(assortmentDict)\n",
    "df1['Assortment'] = df1['Assortment'].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformação da Variável Resposta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A variável resposta 'Sales' é transformada utilizando uma transformação logarítmica (log1p) para reduzir a assimetria e estabilizar a variância dos dados. Essa técnica é valiosa em modelos de regressão, onde se busca uma distribuição normal da variável resposta. Ao aplicar a transformação logarítmica na coluna 'Sales', o modelo se torna mais adequado, minimizando o impacto de outliers e uniformizando a variância ao longo dos valores. A função log1p, que calcula o logaritmo de 1 + x, é utilizada para garantir que valores zero possam ser tratados na transformação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "plt.suptitle('Transformação da Variável Resposta', fontsize=20)\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(df1['Sales'], bins = 50, kde = False)\n",
    "\n",
    "df1['Sales'] = np.log1p(df1['Sales'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.histplot(df1['Sales'], bins = 50, kde = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformação dos Valores Cíclicos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> A transformação de valores cíclicos no arquivo envolve a conversão de atributos como 'DayOfWeek', 'Month', 'Day' e 'WeekOfYear' em representações senoidais e cosseno. Essa técnica é aplicada para preservar a natureza cíclica desses atributos, que não são lineares. Por exemplo, a diferença entre o dia 31 e o dia 1 de um mês é de apenas um dia, embora numericamente seja grande. Ao usar seno e cosseno, os valores próximos no ciclo ficam próximos na representação, permitindo que modelos de machine learning capturem melhor os padrões cíclicos. As fórmulas utilizadas garantem que os valores sejam mapeados para um círculo unitário, onde cada ponto representa uma posição no ciclo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DayOfWeek\n",
    "df1['DayOfWeekSin'] = df1['DayOfWeek'].apply(lambda x: np.sin(x*(2.*np.pi/7)))\n",
    "df1['DayOfWeekCos'] = df1['DayOfWeek'].apply(lambda x: np.cos(x*(2.*np.pi/7)))\n",
    "\n",
    "# Month\n",
    "df1['MonthSin'] = df1['Month'].apply(lambda x: np.sin(x*(2.*np.pi/12)))\n",
    "df1['MonthCos'] = df1['Month'].apply(lambda x: np.cos(x*(2.*np.pi/12)))\n",
    "\n",
    "# Day \n",
    "df1['DaySin'] = df1['Day'].apply(lambda x: np.sin(x*(2.*np.pi/30)))\n",
    "df1['DayCos'] = df1['Day'].apply(lambda x: np.cos(x*(2.*np.pi/30)))\n",
    "\n",
    "# WeekOfYear: o ano possui 52 semanas\n",
    "df1['WeekOfYearSin'] = df1['WeekOfYear'].apply(lambda x: np.sin(x*(2.*np.pi/52))) \n",
    "df1['WeekOfYearCos'] = df1['WeekOfYear'].apply(lambda x: np.cos(x*(2.*np.pi/52)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seleção de Atributos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separação entre Treino e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1.copy()\n",
    "df2 = df2.select_dtypes(exclude=['object'])\n",
    "# cols_drop: colunas redundantes às colunas: \n",
    "# 'Date', 'CompetitionTimeMonth' e 'CompetitionTimeWeek'.\n",
    "cols_drop = [\n",
    "    'WeekOfYear', \n",
    "    'Day', \n",
    "    'Month', \n",
    "    'DayOfWeek', \n",
    "    'PromoSince', \n",
    "    'CompetitionSince'\n",
    "]\n",
    "df2 = df2.drop(cols_drop, axis = 1)\n",
    "\n",
    "# training dataset\n",
    "x_train = df2[df2['Date'] < '2015-06-19']\n",
    "y_train = x_train['Sales']\n",
    "\n",
    "# test dataset - 6 semanas antes da data máxima\n",
    "x_test = df2[df2['Date'] >= '2015-06-19']\n",
    "y_test = x_test['Sales']\n",
    "\n",
    "print('Training Min Date: {}'.format(x_train['Date'].min()))\n",
    "print('Training Max Date: {}'.format(x_train['Date'].max()))\n",
    "\n",
    "print('\\nTest Min Date: {}'.format(x_test['Date'].min()))\n",
    "print('Test Max Date: {}'.format(x_test['Date'].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicação do Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and test dataset for Boruta\n",
    "x_train_n = x_train.drop(['Date', 'Sales'], axis = 1).values # 'x' fit precisa ser array\n",
    "y_train_n = y_train.values.ravel() # 'y' fit precisa ser 1-D array\n",
    "\n",
    "# Define RandomForestRegressor\n",
    "# n_jobs = -1 usa todos os Cores da máquina, \n",
    "# fazendo todo o processamento em paralelo, ou seja, \n",
    "# cria todas as árvores em paralelo e rode mais rápido.\n",
    "rf = RandomForestRegressor(n_jobs = -1)\n",
    "\n",
    "# Define Boruta\n",
    "boruta_file = f\"{path_preprocessing}/featureSelectionModel/feature_selection_boruta.pkl\"\n",
    "\n",
    "if os.path.exists(boruta_file):\n",
    "    # Load Boruta if the file exists\n",
    "    boruta = pkl.load(open(boruta_file, \"rb\"))\n",
    "    print(\"Boruta model loaded from file.\")\n",
    "else:\n",
    "    # Train Boruta if the file does not exist\n",
    "    boruta = BorutaPy(rf, n_estimators = 'auto', verbose = 2, random_state = 42)\n",
    "    boruta.fit(x_train_n, y_train_n)\n",
    "\n",
    "    # Save Boruta\n",
    "    with open(boruta_file, 'wb') as file:\n",
    "        pkl.dump(boruta, file)\n",
    "    print(\"Boruta model trained and saved to file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exibir as features selecionadas pelo Boruta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Capturar todas as features do bourta, reais e shadow features\n",
    "importances = boruta.estimator.feature_importances_\n",
    "\n",
    "# x_train_droped: x_train sem as colunas 'Date' e 'Sales'\n",
    "# Isso é necessário porque o Boruta não considera essas colunas\n",
    "x_train_droped = x_train.drop(\n",
    "    ['Date', 'Sales'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Número de features reais\n",
    "n_features = x_train_droped.shape[1]\n",
    "\n",
    "# Importâncias das features reais\n",
    "real_importances = importances[:n_features]\n",
    "\n",
    "# Nome das features reais\n",
    "feature_names = x_train.drop(\n",
    "    ['Date', 'Sales'], \n",
    "    axis=1\n",
    ").columns\n",
    "\n",
    "# DataFrame com nome e importância\n",
    "importances_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': real_importances,\n",
    "    'Selected': boruta.support_,\n",
    "    'Ranking': boruta.ranking_\n",
    "})\n",
    "\n",
    "# Ordenar por importância decrescente\n",
    "importances_df = importances_df.sort_values(\n",
    "    by='Importance', \n",
    "    ascending=False\n",
    ")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    x='Importance',\n",
    "    y='Feature',\n",
    "    data=importances_df\n",
    ")\n",
    "plt.title('Importância das Features Selecionadas pelo Boruta')\n",
    "plt.xlabel('Ranking (1 = mais importante)')\n",
    "plt.ylabel('Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atributos Selecionados Manualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_features = [\n",
    "    'Store',\n",
    "    'Promo',\n",
    "    'StoreType',\n",
    "    'Assortment',\n",
    "    'CompetitionDistance',\n",
    "    'CompetitionOpenSinceMonth',\n",
    "    'CompetitionOpenSinceYear',\n",
    "    'Promo2',\n",
    "    'Promo2SinceWeek',\n",
    "    'Promo2SinceYear',\n",
    "    'CompetitionTimeMonth',\n",
    "    'PromoTimeWeek',\n",
    "    'DayOfWeekSin',\n",
    "    'DayOfWeekCos',\n",
    "    'MonthSin',\n",
    "    'MonthCos',\n",
    "    'DaySin',\n",
    "    'DayCos',\n",
    "    'WeekOfYearSin',\n",
    "    'WeekOfYearCos']\n",
    "\n",
    "# columns to add\n",
    "feat_to_add = ['Date', 'Sales']\n",
    "\n",
    "list_features_selected = list_features.copy()\n",
    "list_features_selected.extend(feat_to_add)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar as colunas selecionadas manualmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../model/pre-processing/list_features_selected.pkl', 'wb') as file:\n",
    "    pkl.dump(list_features_selected, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar Dados de Treino e Teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFORMED_PATH = '../model/transformed/'\n",
    "train_test_data = (x_train, x_test, y_train, y_test)\n",
    "with open(os.path.join(TRANSFORMED_PATH, 'train_test_data.pkl'), 'wb') as f:\n",
    "    pkl.dump(train_test_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Salvar Scalers e Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROCESSING_PATH = '../model/pre-processing/'\n",
    "\n",
    "scalers = {\n",
    "    'competition_distance_scaler': rs_comp_dist,\n",
    "    'competition_time_month_scaler': rs_comp_time_month,\n",
    "    'promo_time_week_scaler': mms_promo_time_week,\n",
    "    'year_scaler': mms_year\n",
    "}\n",
    "\n",
    "encoders = {\n",
    "    'store_type_encoder': le_store_type\n",
    "}\n",
    "\n",
    "# Salvar dicionário completo de scalers\n",
    "with open(os.path.join(PROCESSING_PATH, 'scalers.pkl'), 'wb') as f:\n",
    "    pkl.dump(scalers, f)\n",
    "\n",
    "# Salvar dicionário completo de encoders\n",
    "with open(os.path.join(PROCESSING_PATH, 'encoders.pkl'), 'wb') as f:\n",
    "    pkl.dump(encoders, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "previsao_vendas_rossmann",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
